{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\"\"\" if(torch.cuda.is_available()):\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\" \"\"\"\n",
    "device =\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nose X</th>\n",
       "      <th>Nose Y</th>\n",
       "      <th>Nose Z</th>\n",
       "      <th>Left Shoulder X</th>\n",
       "      <th>Left Shoulder Y</th>\n",
       "      <th>Left Shoulder Z</th>\n",
       "      <th>Right Shoulder X</th>\n",
       "      <th>Right Shoulder Y</th>\n",
       "      <th>Right Shoulder Z</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>645.819778</td>\n",
       "      <td>228.747883</td>\n",
       "      <td>-495.751920</td>\n",
       "      <td>833.777542</td>\n",
       "      <td>466.400785</td>\n",
       "      <td>-179.431511</td>\n",
       "      <td>453.908920</td>\n",
       "      <td>460.894146</td>\n",
       "      <td>-190.957317</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>645.382538</td>\n",
       "      <td>231.751356</td>\n",
       "      <td>-479.768572</td>\n",
       "      <td>833.793488</td>\n",
       "      <td>460.541124</td>\n",
       "      <td>-157.067810</td>\n",
       "      <td>454.630623</td>\n",
       "      <td>459.233665</td>\n",
       "      <td>-191.068661</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>637.317886</td>\n",
       "      <td>198.295734</td>\n",
       "      <td>-426.740828</td>\n",
       "      <td>833.420715</td>\n",
       "      <td>440.161572</td>\n",
       "      <td>-157.098591</td>\n",
       "      <td>453.729401</td>\n",
       "      <td>428.858485</td>\n",
       "      <td>-178.784831</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>651.948700</td>\n",
       "      <td>228.353620</td>\n",
       "      <td>-363.429108</td>\n",
       "      <td>846.754837</td>\n",
       "      <td>433.019085</td>\n",
       "      <td>-107.618058</td>\n",
       "      <td>496.846695</td>\n",
       "      <td>431.808615</td>\n",
       "      <td>-130.351560</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>658.113556</td>\n",
       "      <td>241.229854</td>\n",
       "      <td>-425.602069</td>\n",
       "      <td>853.639832</td>\n",
       "      <td>431.371436</td>\n",
       "      <td>-108.308308</td>\n",
       "      <td>509.732590</td>\n",
       "      <td>433.925028</td>\n",
       "      <td>-124.991133</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>531.476326</td>\n",
       "      <td>420.706544</td>\n",
       "      <td>-962.552547</td>\n",
       "      <td>782.779236</td>\n",
       "      <td>473.384957</td>\n",
       "      <td>-531.126823</td>\n",
       "      <td>358.240242</td>\n",
       "      <td>511.139517</td>\n",
       "      <td>-479.938474</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>776.463089</td>\n",
       "      <td>350.892634</td>\n",
       "      <td>-545.601182</td>\n",
       "      <td>799.855423</td>\n",
       "      <td>478.437424</td>\n",
       "      <td>-155.256579</td>\n",
       "      <td>471.649704</td>\n",
       "      <td>511.596479</td>\n",
       "      <td>-369.633293</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>762.958908</td>\n",
       "      <td>568.746629</td>\n",
       "      <td>-1356.558838</td>\n",
       "      <td>915.561371</td>\n",
       "      <td>601.033387</td>\n",
       "      <td>-905.601482</td>\n",
       "      <td>473.728752</td>\n",
       "      <td>551.220045</td>\n",
       "      <td>-807.473488</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>745.114288</td>\n",
       "      <td>567.258067</td>\n",
       "      <td>-1074.463663</td>\n",
       "      <td>925.808105</td>\n",
       "      <td>609.548478</td>\n",
       "      <td>-648.498144</td>\n",
       "      <td>473.477173</td>\n",
       "      <td>548.313046</td>\n",
       "      <td>-627.337189</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>709.751129</td>\n",
       "      <td>329.169059</td>\n",
       "      <td>-492.007384</td>\n",
       "      <td>802.841263</td>\n",
       "      <td>501.823282</td>\n",
       "      <td>34.707315</td>\n",
       "      <td>471.204300</td>\n",
       "      <td>464.287806</td>\n",
       "      <td>-207.919607</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Nose X      Nose Y       Nose Z  Left Shoulder X  Left Shoulder Y  \\\n",
       "0   645.819778  228.747883  -495.751920       833.777542       466.400785   \n",
       "1   645.382538  231.751356  -479.768572       833.793488       460.541124   \n",
       "2   637.317886  198.295734  -426.740828       833.420715       440.161572   \n",
       "3   651.948700  228.353620  -363.429108       846.754837       433.019085   \n",
       "4   658.113556  241.229854  -425.602069       853.639832       431.371436   \n",
       "..         ...         ...          ...              ...              ...   \n",
       "71  531.476326  420.706544  -962.552547       782.779236       473.384957   \n",
       "72  776.463089  350.892634  -545.601182       799.855423       478.437424   \n",
       "73  762.958908  568.746629 -1356.558838       915.561371       601.033387   \n",
       "74  745.114288  567.258067 -1074.463663       925.808105       609.548478   \n",
       "75  709.751129  329.169059  -492.007384       802.841263       501.823282   \n",
       "\n",
       "    Left Shoulder Z  Right Shoulder X  Right Shoulder Y  Right Shoulder Z  \\\n",
       "0       -179.431511        453.908920        460.894146       -190.957317   \n",
       "1       -157.067810        454.630623        459.233665       -191.068661   \n",
       "2       -157.098591        453.729401        428.858485       -178.784831   \n",
       "3       -107.618058        496.846695        431.808615       -130.351560   \n",
       "4       -108.308308        509.732590        433.925028       -124.991133   \n",
       "..              ...               ...               ...               ...   \n",
       "71      -531.126823        358.240242        511.139517       -479.938474   \n",
       "72      -155.256579        471.649704        511.596479       -369.633293   \n",
       "73      -905.601482        473.728752        551.220045       -807.473488   \n",
       "74      -648.498144        473.477173        548.313046       -627.337189   \n",
       "75        34.707315        471.204300        464.287806       -207.919607   \n",
       "\n",
       "   Label  \n",
       "0   good  \n",
       "1   good  \n",
       "2   good  \n",
       "3   good  \n",
       "4   good  \n",
       "..   ...  \n",
       "71   bad  \n",
       "72   bad  \n",
       "73   bad  \n",
       "74   bad  \n",
       "75   bad  \n",
       "\n",
       "[142 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1=pd.read_csv(\"./data/goodtr.csv\")\n",
    "data2=pd.read_csv(\"./data/badtr.csv\")\n",
    "\n",
    "data_train = pd.concat([data1,data2])\n",
    "data_train.head(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nose X</th>\n",
       "      <th>Nose Y</th>\n",
       "      <th>Nose Z</th>\n",
       "      <th>Left Shoulder X</th>\n",
       "      <th>Left Shoulder Y</th>\n",
       "      <th>Left Shoulder Z</th>\n",
       "      <th>Right Shoulder X</th>\n",
       "      <th>Right Shoulder Y</th>\n",
       "      <th>Right Shoulder Z</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>629.449768</td>\n",
       "      <td>287.503946</td>\n",
       "      <td>-433.363738</td>\n",
       "      <td>804.088211</td>\n",
       "      <td>473.737378</td>\n",
       "      <td>-177.699909</td>\n",
       "      <td>483.490562</td>\n",
       "      <td>482.585363</td>\n",
       "      <td>-174.759039</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>703.323135</td>\n",
       "      <td>287.549565</td>\n",
       "      <td>-363.598237</td>\n",
       "      <td>879.282303</td>\n",
       "      <td>479.812174</td>\n",
       "      <td>-168.614194</td>\n",
       "      <td>528.779068</td>\n",
       "      <td>483.545852</td>\n",
       "      <td>-141.752150</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>656.710587</td>\n",
       "      <td>202.545190</td>\n",
       "      <td>-441.115794</td>\n",
       "      <td>867.903748</td>\n",
       "      <td>429.677567</td>\n",
       "      <td>-206.222520</td>\n",
       "      <td>464.457474</td>\n",
       "      <td>429.084349</td>\n",
       "      <td>-195.042815</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>736.965332</td>\n",
       "      <td>278.175781</td>\n",
       "      <td>-723.139172</td>\n",
       "      <td>911.553497</td>\n",
       "      <td>470.187292</td>\n",
       "      <td>-381.313992</td>\n",
       "      <td>519.369125</td>\n",
       "      <td>462.966356</td>\n",
       "      <td>-415.945730</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>664.094315</td>\n",
       "      <td>245.109444</td>\n",
       "      <td>-588.598452</td>\n",
       "      <td>900.551300</td>\n",
       "      <td>461.928535</td>\n",
       "      <td>-284.289458</td>\n",
       "      <td>472.788200</td>\n",
       "      <td>459.991164</td>\n",
       "      <td>-292.915313</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>698.764801</td>\n",
       "      <td>268.351686</td>\n",
       "      <td>-703.565011</td>\n",
       "      <td>929.103317</td>\n",
       "      <td>479.501209</td>\n",
       "      <td>-411.444726</td>\n",
       "      <td>472.503510</td>\n",
       "      <td>483.020782</td>\n",
       "      <td>-381.039248</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>696.363297</td>\n",
       "      <td>284.372070</td>\n",
       "      <td>-602.703695</td>\n",
       "      <td>911.474686</td>\n",
       "      <td>500.466771</td>\n",
       "      <td>-321.058617</td>\n",
       "      <td>473.740311</td>\n",
       "      <td>507.648783</td>\n",
       "      <td>-309.163857</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>695.806046</td>\n",
       "      <td>272.200098</td>\n",
       "      <td>-573.550444</td>\n",
       "      <td>899.144821</td>\n",
       "      <td>498.546610</td>\n",
       "      <td>-305.249355</td>\n",
       "      <td>473.371544</td>\n",
       "      <td>504.808345</td>\n",
       "      <td>-290.026531</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>695.218887</td>\n",
       "      <td>267.044764</td>\n",
       "      <td>-468.708386</td>\n",
       "      <td>893.526230</td>\n",
       "      <td>498.680248</td>\n",
       "      <td>-202.003512</td>\n",
       "      <td>472.515564</td>\n",
       "      <td>502.729268</td>\n",
       "      <td>-240.985043</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>694.290161</td>\n",
       "      <td>269.613333</td>\n",
       "      <td>-425.918226</td>\n",
       "      <td>882.824860</td>\n",
       "      <td>493.672800</td>\n",
       "      <td>-102.939910</td>\n",
       "      <td>472.936592</td>\n",
       "      <td>495.817537</td>\n",
       "      <td>-129.907740</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>687.065430</td>\n",
       "      <td>270.851827</td>\n",
       "      <td>-484.600496</td>\n",
       "      <td>870.432129</td>\n",
       "      <td>486.569066</td>\n",
       "      <td>-162.843068</td>\n",
       "      <td>471.314888</td>\n",
       "      <td>491.385970</td>\n",
       "      <td>-170.920894</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>686.675873</td>\n",
       "      <td>274.360499</td>\n",
       "      <td>-487.687955</td>\n",
       "      <td>868.487625</td>\n",
       "      <td>482.185135</td>\n",
       "      <td>-133.509529</td>\n",
       "      <td>471.683884</td>\n",
       "      <td>487.159195</td>\n",
       "      <td>-173.767372</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>707.260056</td>\n",
       "      <td>270.358000</td>\n",
       "      <td>-352.955532</td>\n",
       "      <td>881.438293</td>\n",
       "      <td>465.505571</td>\n",
       "      <td>-109.882658</td>\n",
       "      <td>524.394760</td>\n",
       "      <td>466.933279</td>\n",
       "      <td>-144.217680</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>707.351532</td>\n",
       "      <td>241.965508</td>\n",
       "      <td>-380.730987</td>\n",
       "      <td>885.531845</td>\n",
       "      <td>449.576125</td>\n",
       "      <td>-107.476727</td>\n",
       "      <td>526.921692</td>\n",
       "      <td>453.422670</td>\n",
       "      <td>-156.975918</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>705.279541</td>\n",
       "      <td>245.486884</td>\n",
       "      <td>-609.721169</td>\n",
       "      <td>894.631653</td>\n",
       "      <td>459.188776</td>\n",
       "      <td>-217.472219</td>\n",
       "      <td>513.574829</td>\n",
       "      <td>453.962803</td>\n",
       "      <td>-285.283763</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>653.071899</td>\n",
       "      <td>247.594864</td>\n",
       "      <td>-550.971308</td>\n",
       "      <td>877.442474</td>\n",
       "      <td>451.814075</td>\n",
       "      <td>-158.336785</td>\n",
       "      <td>479.754066</td>\n",
       "      <td>456.363487</td>\n",
       "      <td>-157.854846</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>632.740364</td>\n",
       "      <td>245.195210</td>\n",
       "      <td>-579.277067</td>\n",
       "      <td>845.949173</td>\n",
       "      <td>432.697005</td>\n",
       "      <td>-194.650676</td>\n",
       "      <td>437.650948</td>\n",
       "      <td>429.533114</td>\n",
       "      <td>-231.338661</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>715.415955</td>\n",
       "      <td>584.799371</td>\n",
       "      <td>-1032.254019</td>\n",
       "      <td>932.152252</td>\n",
       "      <td>583.881583</td>\n",
       "      <td>-685.010691</td>\n",
       "      <td>519.674759</td>\n",
       "      <td>569.154925</td>\n",
       "      <td>-617.172647</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>713.711777</td>\n",
       "      <td>620.993657</td>\n",
       "      <td>-1183.147202</td>\n",
       "      <td>966.189499</td>\n",
       "      <td>688.436537</td>\n",
       "      <td>-787.063293</td>\n",
       "      <td>488.173752</td>\n",
       "      <td>689.148030</td>\n",
       "      <td>-748.276234</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>713.948288</td>\n",
       "      <td>530.468245</td>\n",
       "      <td>-1773.930130</td>\n",
       "      <td>970.634689</td>\n",
       "      <td>663.481092</td>\n",
       "      <td>-1115.320444</td>\n",
       "      <td>451.891251</td>\n",
       "      <td>649.173503</td>\n",
       "      <td>-1062.535086</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>638.851509</td>\n",
       "      <td>289.288795</td>\n",
       "      <td>-602.552247</td>\n",
       "      <td>884.342804</td>\n",
       "      <td>541.427493</td>\n",
       "      <td>-260.339241</td>\n",
       "      <td>422.857933</td>\n",
       "      <td>495.865560</td>\n",
       "      <td>-253.945241</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>874.974289</td>\n",
       "      <td>385.239244</td>\n",
       "      <td>-677.078733</td>\n",
       "      <td>981.578522</td>\n",
       "      <td>617.714410</td>\n",
       "      <td>-278.460975</td>\n",
       "      <td>499.218712</td>\n",
       "      <td>498.728099</td>\n",
       "      <td>-286.509769</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>764.705887</td>\n",
       "      <td>484.023886</td>\n",
       "      <td>-1317.066679</td>\n",
       "      <td>989.374161</td>\n",
       "      <td>615.797725</td>\n",
       "      <td>-885.342951</td>\n",
       "      <td>434.962082</td>\n",
       "      <td>556.481638</td>\n",
       "      <td>-804.149609</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>724.638290</td>\n",
       "      <td>538.250170</td>\n",
       "      <td>-1515.778542</td>\n",
       "      <td>998.366547</td>\n",
       "      <td>621.942258</td>\n",
       "      <td>-965.637903</td>\n",
       "      <td>424.161911</td>\n",
       "      <td>579.224238</td>\n",
       "      <td>-949.406290</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>698.160477</td>\n",
       "      <td>477.343340</td>\n",
       "      <td>-1214.696760</td>\n",
       "      <td>948.744431</td>\n",
       "      <td>619.913993</td>\n",
       "      <td>-490.740395</td>\n",
       "      <td>395.424576</td>\n",
       "      <td>614.805179</td>\n",
       "      <td>-587.035646</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>698.258133</td>\n",
       "      <td>425.036402</td>\n",
       "      <td>-773.655252</td>\n",
       "      <td>947.968597</td>\n",
       "      <td>607.338724</td>\n",
       "      <td>-359.512696</td>\n",
       "      <td>413.366432</td>\n",
       "      <td>596.846137</td>\n",
       "      <td>-378.338628</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>680.365601</td>\n",
       "      <td>406.542034</td>\n",
       "      <td>-504.312415</td>\n",
       "      <td>910.319366</td>\n",
       "      <td>490.484018</td>\n",
       "      <td>-169.208797</td>\n",
       "      <td>423.553505</td>\n",
       "      <td>487.494578</td>\n",
       "      <td>-159.099455</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>672.736130</td>\n",
       "      <td>429.907851</td>\n",
       "      <td>-846.887455</td>\n",
       "      <td>908.255310</td>\n",
       "      <td>496.330290</td>\n",
       "      <td>-400.374498</td>\n",
       "      <td>430.462265</td>\n",
       "      <td>488.420134</td>\n",
       "      <td>-375.255504</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>658.984833</td>\n",
       "      <td>437.216721</td>\n",
       "      <td>-723.784962</td>\n",
       "      <td>905.844803</td>\n",
       "      <td>485.018620</td>\n",
       "      <td>-318.925316</td>\n",
       "      <td>434.180527</td>\n",
       "      <td>487.090530</td>\n",
       "      <td>-304.167073</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>675.761261</td>\n",
       "      <td>344.786167</td>\n",
       "      <td>-527.867231</td>\n",
       "      <td>902.624664</td>\n",
       "      <td>502.914748</td>\n",
       "      <td>-132.526048</td>\n",
       "      <td>473.751259</td>\n",
       "      <td>484.754047</td>\n",
       "      <td>-211.451926</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>678.615646</td>\n",
       "      <td>254.436944</td>\n",
       "      <td>-411.370955</td>\n",
       "      <td>878.011169</td>\n",
       "      <td>455.594444</td>\n",
       "      <td>-58.105198</td>\n",
       "      <td>484.550743</td>\n",
       "      <td>436.563034</td>\n",
       "      <td>-167.192806</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>712.308655</td>\n",
       "      <td>318.939049</td>\n",
       "      <td>-1072.456255</td>\n",
       "      <td>921.236267</td>\n",
       "      <td>469.815860</td>\n",
       "      <td>-572.850323</td>\n",
       "      <td>468.493729</td>\n",
       "      <td>459.266796</td>\n",
       "      <td>-594.495964</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>699.779816</td>\n",
       "      <td>284.899178</td>\n",
       "      <td>-906.171570</td>\n",
       "      <td>930.817108</td>\n",
       "      <td>476.032577</td>\n",
       "      <td>-506.574697</td>\n",
       "      <td>460.994644</td>\n",
       "      <td>472.813840</td>\n",
       "      <td>-474.027185</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>716.220551</td>\n",
       "      <td>304.786642</td>\n",
       "      <td>-714.448128</td>\n",
       "      <td>929.028168</td>\n",
       "      <td>516.103878</td>\n",
       "      <td>-283.389266</td>\n",
       "      <td>491.489029</td>\n",
       "      <td>478.787570</td>\n",
       "      <td>-371.115847</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Nose X      Nose Y       Nose Z  Left Shoulder X  Left Shoulder Y  \\\n",
       "0   629.449768  287.503946  -433.363738       804.088211       473.737378   \n",
       "1   703.323135  287.549565  -363.598237       879.282303       479.812174   \n",
       "2   656.710587  202.545190  -441.115794       867.903748       429.677567   \n",
       "3   736.965332  278.175781  -723.139172       911.553497       470.187292   \n",
       "4   664.094315  245.109444  -588.598452       900.551300       461.928535   \n",
       "5   698.764801  268.351686  -703.565011       929.103317       479.501209   \n",
       "6   696.363297  284.372070  -602.703695       911.474686       500.466771   \n",
       "7   695.806046  272.200098  -573.550444       899.144821       498.546610   \n",
       "8   695.218887  267.044764  -468.708386       893.526230       498.680248   \n",
       "9   694.290161  269.613333  -425.918226       882.824860       493.672800   \n",
       "10  687.065430  270.851827  -484.600496       870.432129       486.569066   \n",
       "11  686.675873  274.360499  -487.687955       868.487625       482.185135   \n",
       "12  707.260056  270.358000  -352.955532       881.438293       465.505571   \n",
       "13  707.351532  241.965508  -380.730987       885.531845       449.576125   \n",
       "14  705.279541  245.486884  -609.721169       894.631653       459.188776   \n",
       "15  653.071899  247.594864  -550.971308       877.442474       451.814075   \n",
       "16  632.740364  245.195210  -579.277067       845.949173       432.697005   \n",
       "0   715.415955  584.799371 -1032.254019       932.152252       583.881583   \n",
       "1   713.711777  620.993657 -1183.147202       966.189499       688.436537   \n",
       "2   713.948288  530.468245 -1773.930130       970.634689       663.481092   \n",
       "3   638.851509  289.288795  -602.552247       884.342804       541.427493   \n",
       "4   874.974289  385.239244  -677.078733       981.578522       617.714410   \n",
       "5   764.705887  484.023886 -1317.066679       989.374161       615.797725   \n",
       "6   724.638290  538.250170 -1515.778542       998.366547       621.942258   \n",
       "7   698.160477  477.343340 -1214.696760       948.744431       619.913993   \n",
       "8   698.258133  425.036402  -773.655252       947.968597       607.338724   \n",
       "9   680.365601  406.542034  -504.312415       910.319366       490.484018   \n",
       "10  672.736130  429.907851  -846.887455       908.255310       496.330290   \n",
       "11  658.984833  437.216721  -723.784962       905.844803       485.018620   \n",
       "12  675.761261  344.786167  -527.867231       902.624664       502.914748   \n",
       "13  678.615646  254.436944  -411.370955       878.011169       455.594444   \n",
       "14  712.308655  318.939049 -1072.456255       921.236267       469.815860   \n",
       "15  699.779816  284.899178  -906.171570       930.817108       476.032577   \n",
       "16  716.220551  304.786642  -714.448128       929.028168       516.103878   \n",
       "\n",
       "    Left Shoulder Z  Right Shoulder X  Right Shoulder Y  Right Shoulder Z  \\\n",
       "0       -177.699909        483.490562        482.585363       -174.759039   \n",
       "1       -168.614194        528.779068        483.545852       -141.752150   \n",
       "2       -206.222520        464.457474        429.084349       -195.042815   \n",
       "3       -381.313992        519.369125        462.966356       -415.945730   \n",
       "4       -284.289458        472.788200        459.991164       -292.915313   \n",
       "5       -411.444726        472.503510        483.020782       -381.039248   \n",
       "6       -321.058617        473.740311        507.648783       -309.163857   \n",
       "7       -305.249355        473.371544        504.808345       -290.026531   \n",
       "8       -202.003512        472.515564        502.729268       -240.985043   \n",
       "9       -102.939910        472.936592        495.817537       -129.907740   \n",
       "10      -162.843068        471.314888        491.385970       -170.920894   \n",
       "11      -133.509529        471.683884        487.159195       -173.767372   \n",
       "12      -109.882658        524.394760        466.933279       -144.217680   \n",
       "13      -107.476727        526.921692        453.422670       -156.975918   \n",
       "14      -217.472219        513.574829        453.962803       -285.283763   \n",
       "15      -158.336785        479.754066        456.363487       -157.854846   \n",
       "16      -194.650676        437.650948        429.533114       -231.338661   \n",
       "0       -685.010691        519.674759        569.154925       -617.172647   \n",
       "1       -787.063293        488.173752        689.148030       -748.276234   \n",
       "2      -1115.320444        451.891251        649.173503      -1062.535086   \n",
       "3       -260.339241        422.857933        495.865560       -253.945241   \n",
       "4       -278.460975        499.218712        498.728099       -286.509769   \n",
       "5       -885.342951        434.962082        556.481638       -804.149609   \n",
       "6       -965.637903        424.161911        579.224238       -949.406290   \n",
       "7       -490.740395        395.424576        614.805179       -587.035646   \n",
       "8       -359.512696        413.366432        596.846137       -378.338628   \n",
       "9       -169.208797        423.553505        487.494578       -159.099455   \n",
       "10      -400.374498        430.462265        488.420134       -375.255504   \n",
       "11      -318.925316        434.180527        487.090530       -304.167073   \n",
       "12      -132.526048        473.751259        484.754047       -211.451926   \n",
       "13       -58.105198        484.550743        436.563034       -167.192806   \n",
       "14      -572.850323        468.493729        459.266796       -594.495964   \n",
       "15      -506.574697        460.994644        472.813840       -474.027185   \n",
       "16      -283.389266        491.489029        478.787570       -371.115847   \n",
       "\n",
       "   Label  \n",
       "0   good  \n",
       "1   good  \n",
       "2   good  \n",
       "3   good  \n",
       "4   good  \n",
       "5   good  \n",
       "6   good  \n",
       "7   good  \n",
       "8   good  \n",
       "9   good  \n",
       "10  good  \n",
       "11  good  \n",
       "12  good  \n",
       "13  good  \n",
       "14  good  \n",
       "15  good  \n",
       "16  good  \n",
       "0    bad  \n",
       "1    bad  \n",
       "2    bad  \n",
       "3    bad  \n",
       "4    bad  \n",
       "5    bad  \n",
       "6    bad  \n",
       "7    bad  \n",
       "8    bad  \n",
       "9    bad  \n",
       "10   bad  \n",
       "11   bad  \n",
       "12   bad  \n",
       "13   bad  \n",
       "14   bad  \n",
       "15   bad  \n",
       "16   bad  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data3=pd.read_csv(\"./data/goodval.csv\")\n",
    "data4=pd.read_csv(\"./data/badval.csv\")\n",
    "\n",
    "data_test = pd.concat([data3,data4])\n",
    "data_test.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_train,data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "import torch.utils\n",
    "from pickle import dump\n",
    "encoder = LabelEncoder()\n",
    "data[\"Label\"] = encoder.fit_transform(data[\"Label\"])\n",
    "X=data.drop(columns=\"Label\").to_numpy(dtype=np.float32)\n",
    "y=data[\"Label\"].to_numpy(dtype=np.float32)\n",
    "#scaler=MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "#scaler.fit(X_train)\n",
    "#X_train = scaler.transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "train_dataset= torch.utils.data.TensorDataset(torch.tensor(X_train),torch.tensor(y_train))\n",
    "\n",
    "test_dataset= torch.utils.data.TensorDataset(torch.tensor(X_test),torch.tensor(y_test))\n",
    "#dump(scaler, open('scaler.pkl', 'wb'))\n",
    "\n",
    "train_dataloader=torch.utils.data.DataLoader(train_dataset,batch_size=3,num_workers=3)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=1,num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class slouch_detection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(slouch_detection,self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.LayerNorm(9),\n",
    "            nn.Linear(9,36),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(36,15),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(15,12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12,6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6,3),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    @torch.compile(fullgraph=True, dynamic=True, options = {\"epilogue_fusion\": True, \"max_autotune\": True,\n",
    "                    \"shape_padding\": True})\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = slouch_detection().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Assuming you're using Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1228"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total=0\n",
    "for params in model.parameters():\n",
    "    total+=params.numel()\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, epochs,device,best_acc):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            labels=labels.unsqueeze(dim=1).to(device)\n",
    "            outputs = model(inputs.to(device))\n",
    "            #print(outputs.size(),labels.unsqueeze(dim=1).size())\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted = torch.round(outputs)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            print()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "        test_model(model, test_dataloader, criterion,device,best_acc)\n",
    "\n",
    "# Testing loop\n",
    "def test_model(model, test_loader, criterion,device,best_acc):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            labels=labels.unsqueeze(dim=1).to(device)\n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            predicted = torch.round(outputs)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        test_loss = running_loss / len(test_loader.dataset)\n",
    "        test_acc = correct / total\n",
    "        if(best_acc<test_acc):\n",
    "            torch.save(model.state_dict(),\"slouch_detector.pt\")\n",
    "            best_acc=test_acc\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/50, Loss: 0.6974, Accuracy: 0.5385\n",
      "Test Loss: 0.7082, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2/50, Loss: 0.6966, Accuracy: 0.5385\n",
      "Test Loss: 0.7075, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 3/50, Loss: 0.6962, Accuracy: 0.5385\n",
      "Test Loss: 0.7069, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 4/50, Loss: 0.6958, Accuracy: 0.5385\n",
      "Test Loss: 0.7062, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 5/50, Loss: 0.6954, Accuracy: 0.5385\n",
      "Test Loss: 0.7057, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 6/50, Loss: 0.6950, Accuracy: 0.5385\n",
      "Test Loss: 0.7052, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 7/50, Loss: 0.6947, Accuracy: 0.5385\n",
      "Test Loss: 0.7046, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 8/50, Loss: 0.6943, Accuracy: 0.5385\n",
      "Test Loss: 0.7040, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 9/50, Loss: 0.6933, Accuracy: 0.5385\n",
      "Test Loss: 0.7027, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 10/50, Loss: 0.6889, Accuracy: 0.5385\n",
      "Test Loss: 0.6956, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 11/50, Loss: 0.6669, Accuracy: 0.5385\n",
      "Test Loss: 0.6715, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 12/50, Loss: 0.6220, Accuracy: 0.5385\n",
      "Test Loss: 0.6276, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 13/50, Loss: 0.5815, Accuracy: 0.5385\n",
      "Test Loss: 0.6016, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 14/50, Loss: 0.5601, Accuracy: 0.5385\n",
      "Test Loss: 0.5839, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 15/50, Loss: 0.5493, Accuracy: 0.5385\n",
      "Test Loss: 0.5780, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 16/50, Loss: 0.5426, Accuracy: 0.5385\n",
      "Test Loss: 0.5702, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 17/50, Loss: 0.5332, Accuracy: 0.5385\n",
      "Test Loss: 0.5663, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 18/50, Loss: 0.5288, Accuracy: 0.5385\n",
      "Test Loss: 0.5601, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 19/50, Loss: 0.5243, Accuracy: 0.5385\n",
      "Test Loss: 0.5645, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 20/50, Loss: 0.5181, Accuracy: 0.5385\n",
      "Test Loss: 0.5610, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 21/50, Loss: 0.5139, Accuracy: 0.5385\n",
      "Test Loss: 0.5578, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 22/50, Loss: 0.5100, Accuracy: 0.5385\n",
      "Test Loss: 0.5563, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 23/50, Loss: 0.5057, Accuracy: 0.5385\n",
      "Test Loss: 0.5525, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 24/50, Loss: 0.5020, Accuracy: 0.5385\n",
      "Test Loss: 0.5494, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 25/50, Loss: 0.4984, Accuracy: 0.5385\n",
      "Test Loss: 0.5479, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 26/50, Loss: 0.4958, Accuracy: 0.5385\n",
      "Test Loss: 0.5465, Test Accuracy: 0.5085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 27/50, Loss: 0.4926, Accuracy: 0.6581\n",
      "Test Loss: 0.5436, Test Accuracy: 0.7458\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 28/50, Loss: 0.4894, Accuracy: 0.8291\n",
      "Test Loss: 0.5406, Test Accuracy: 0.7458\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 29/50, Loss: 0.4861, Accuracy: 0.8291\n",
      "Test Loss: 0.5380, Test Accuracy: 0.7627\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 30/50, Loss: 0.4831, Accuracy: 0.8291\n",
      "Test Loss: 0.5351, Test Accuracy: 0.7627\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 31/50, Loss: 0.4807, Accuracy: 0.8291\n",
      "Test Loss: 0.5310, Test Accuracy: 0.7119\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 32/50, Loss: 0.4776, Accuracy: 0.8376\n",
      "Test Loss: 0.5287, Test Accuracy: 0.7119\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 33/50, Loss: 0.4749, Accuracy: 0.8376\n",
      "Test Loss: 0.5262, Test Accuracy: 0.7119\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 34/50, Loss: 0.4716, Accuracy: 0.8376\n",
      "Test Loss: 0.5252, Test Accuracy: 0.7458\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 35/50, Loss: 0.4698, Accuracy: 0.8291\n",
      "Test Loss: 0.5212, Test Accuracy: 0.7288\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 36/50, Loss: 0.4667, Accuracy: 0.8376\n",
      "Test Loss: 0.5194, Test Accuracy: 0.7288\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 37/50, Loss: 0.4649, Accuracy: 0.8376\n",
      "Test Loss: 0.5158, Test Accuracy: 0.7119\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 38/50, Loss: 0.4623, Accuracy: 0.8462\n",
      "Test Loss: 0.5139, Test Accuracy: 0.7288\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 39/50, Loss: 0.4600, Accuracy: 0.8462\n",
      "Test Loss: 0.5118, Test Accuracy: 0.7288\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 40/50, Loss: 0.4567, Accuracy: 0.8462\n",
      "Test Loss: 0.5113, Test Accuracy: 0.7458\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 41/50, Loss: 0.4557, Accuracy: 0.8376\n",
      "Test Loss: 0.5074, Test Accuracy: 0.7119\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 42/50, Loss: 0.4529, Accuracy: 0.8462\n",
      "Test Loss: 0.5064, Test Accuracy: 0.7288\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 43/50, Loss: 0.4512, Accuracy: 0.8462\n",
      "Test Loss: 0.5039, Test Accuracy: 0.7119\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 44/50, Loss: 0.4491, Accuracy: 0.8462\n",
      "Test Loss: 0.5021, Test Accuracy: 0.7119\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 45/50, Loss: 0.4471, Accuracy: 0.8462\n",
      "Test Loss: 0.5004, Test Accuracy: 0.7119\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 46/50, Loss: 0.4451, Accuracy: 0.8462\n",
      "Test Loss: 0.4986, Test Accuracy: 0.7119\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 47/50, Loss: 0.4432, Accuracy: 0.8462\n",
      "Test Loss: 0.4969, Test Accuracy: 0.7119\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 48/50, Loss: 0.4414, Accuracy: 0.8462\n",
      "Test Loss: 0.4951, Test Accuracy: 0.7119\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 49/50, Loss: 0.4395, Accuracy: 0.8462\n",
      "Test Loss: 0.4934, Test Accuracy: 0.7119\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 50/50, Loss: 0.4378, Accuracy: 0.8462\n",
      "Test Loss: 0.4918, Test Accuracy: 0.7119\n",
      "Test Loss: 0.4918, Test Accuracy: 0.7119\n"
     ]
    }
   ],
   "source": [
    "best_acc=0\n",
    "train_model(model, train_dataloader, optimizer, criterion,device=device, epochs=50,best_acc=best_acc)  # Adjust epochs as needed\n",
    "\n",
    "test_model(model, test_dataloader, criterion,device,best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4918, Test Accuracy: 0.7119\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_dataloader, criterion,device,best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
